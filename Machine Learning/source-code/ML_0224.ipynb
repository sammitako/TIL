{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 상수노드 생성 (자주 사용 안됨)\n",
    "node1 = tf.constant(10, dtype=tf.float32) \n",
    "node2 = tf.constant(20, dtype=tf.float32) \n",
    "\n",
    "# 수치연산\n",
    "node3 = node1 + node2 \n",
    "\n",
    "# 그래프 실행\n",
    "sess = tf.Session() # 2.x버전에서는 session이 삭제됨\n",
    "print(sess.run(node3)) # 30.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    }
   ],
   "source": [
    "# Data Flow Graph에 입력값 할당 시, placeholder를 이용\n",
    "import tensorflow as tf\n",
    "\n",
    "node1 = tf.placeholder(dtype=tf.float32) # 입력 파라미터를 \"받아주는\" 바구니\n",
    "node2 = tf.placeholder(dtype=tf.float32)\n",
    "node3 = node1 + node2\n",
    "\n",
    "sess = tf.Session()\n",
    "result = sess.run(node3, feed_dict={node1: 10, node2: 20})\n",
    "print(result) # 30.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]]\n",
      "W: [[0.26164162]], b: [1.3429476], loss: 31.237262725830078\n",
      "W: [[1.9230976]], b: [1.2776417], loss: 0.014040273614227772\n",
      "W: [[1.9721142]], b: [1.1006749], loss: 0.0018460738938301802\n",
      "W: [[1.9898862]], b: [1.0365115], loss: 0.00024281651712954044\n",
      "W: [[1.9963293]], b: [1.0132507], loss: 3.198089325451292e-05\n",
      "W: [[1.9986664]], b: [1.004817], loss: 4.2258411667717155e-06\n",
      "W: [[1.9995133]], b: [1.001755], loss: 5.610386324406136e-07\n",
      "W: [[1.9998204]], b: [1.0006487], loss: 7.667426871194039e-08\n",
      "W: [[1.9999334]], b: [1.0002346], loss: 1.008385197565076e-08\n",
      "W: [[1.9999406]], b: [1.0002079], loss: 7.925575751244196e-09\n",
      "예측값: [[18.999674]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np       # 자료구조\n",
    "import pandas as pd      # 데이터핸들링\n",
    "import tensorflow as tf  # 머신러닝 \n",
    "\n",
    "\n",
    "# 1. Training Data Set - 입력 데이터\n",
    "# 1차원 벡터 -> 2차원 Matrix\n",
    "x_data = (np.array([1,2,3,4,5])).reshape(5,1)  # 독립변수 1개; 공부시간\n",
    "t_data = (np.array([3,5,7,9,11])).reshape(5,1) # lable; 시험성적\n",
    "print(x_data)\n",
    "\n",
    "\n",
    "# 2. Data -> Data Flow Graph => placeholder 사용\n",
    "# 입력 데이터가 2차원 이상일 경우, 차원을 명시해야 한다.\n",
    "# placeholder: 예측 모델이 완성된 후에는 미지의 X를 입력하여 예측값을 도출하는 용도로 사용\n",
    "# 따라서 shape=[None, 1]: 2차원 Matrix이고, 컬럼의 갯수가 한 개, 대신 레코드의 수는 상관 없음\n",
    "X = tf.placeholder(shape=[None,1], dtype=tf.float32) \n",
    "T = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# 3. Hypothesis : 일차함수 (y = Wx + b)\n",
    "\n",
    "# (3-1). Weight ,bias - 계속 변하는 값\n",
    "# Simple Linear Regression이므로 독립변수가 한개니까 W도 한 개만 구하면 된다.\n",
    "W = tf.Variable(tf.random.normal([1,1]), name='weight') # 랜덤한 초기값 \n",
    "                                                        # 나중에 그래프가 반복적으로 수행되면서 값이 갱신됨\n",
    "                                                        # [1,1]: 2차원 Matrix 형태로 W \"한개\" 구함\n",
    "\n",
    "# [1]: 1차원 벡터 형태로 W \"한개\" 구함\n",
    "# 행렬곱 연산을 안하므로 굳이 2차원 Matrix 형태로 구할 필요 없음\n",
    "b = tf.Variable(tf.random.normal([1]), name='bias')  \n",
    "\n",
    "\n",
    "# (3-2). Predict Model 정의 => 일차함수를 행렬식으로 변환: y = X dot W + b\n",
    "## X, W: 2차원 Matrix\n",
    "## b: 스칼라 (broadcasting)\n",
    "\n",
    "H = tf.matmul(X, W) + b # y = Wx + b => 2차원 행렬로 처리 => y = X dot W + b\n",
    "                        # y = 예측값, prediction model\n",
    "    \n",
    "# 4. W,b를 구하기 위해 평균제곱오차를 이용한 최소제곱법을 통해 손실함수 정의\n",
    "loss = tf.reduce_mean(tf.square(H - T)) # 행렬차에 대해 각각의 값을 제곱 후 평균구함\n",
    "\n",
    "\n",
    "# 5. 반복 학습 진행 - 손실함수를 미분해나가면서 W, b를 갱신 \n",
    "\n",
    "# (5-1). 경사하강법 알고리즘을 수행할 학습용 노드 생성\n",
    "#       : (미분을 통해) 손실함수의 값을 줄여가면서 W, b의 값을 1번 갱신\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-3).minimize(loss) \n",
    "\n",
    "# (5-2). 위의 (5-1)의 작업을 반복 수행하여 최적의 W, b를 구함 -> # 7으로 이동\n",
    "\n",
    "\n",
    "# 6. Tensorflow 그래프 실행\n",
    "sess = tf.Session() # Session 얻어옴\n",
    "sess.run(tf.global_variables_initializer()) # 전역변수 초기화 (2.x 버전에서는 불필요)\n",
    "\n",
    " \n",
    "# 7. 반복 학습 진행 - 1 epoch: Training Data Set 전체를 이용하여 한 번 학습\n",
    "for step in range(30000): # 3000 epoch\n",
    "    \n",
    "    # 학습용 - train 노드만 실행하면 모든 하위노드들이 실행됨\n",
    "    # 확인용 - 구해야하는 값: W, b 그리고 손실함수 값: 0과 가까워질 만큼 작아져야함\n",
    "    _, W_val, b_val, loss_val  = sess.run([train, W, b, loss], feed_dict={X: x_data, T: t_data}) # _: 리턴값 사용 안함\n",
    "    \n",
    "    \n",
    "    if step%3000 == 0: # 10번만 출력\n",
    "        # y = 2x + 1, 따라서 W = 2, b = 1\n",
    "        print('W: {}, b: {}, loss: {}'.format(W_val, b_val, loss_val))\n",
    "        \n",
    "        \n",
    "# 8. 학습 종료 후 최적의 W, b가 계산되면 이를 이용하여 모델(H)이 완성됨\n",
    "#    이제, Prediction(예측) 작업을 실행해보자.\n",
    "\n",
    "result = sess.run(H, feed_dict={X: [[9]]}) # X: 2차원 Matrix 형태이므로 중첩 리스트(1행1열)로 구현\n",
    "print('예측값: {}'.format(result)) # 18.999674"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [[0.58561806]], b: [0.40027785]\n",
      "W: [[2.0194011]], b: [0.92995587]\n",
      "W: [[2.00703589]], b: [0.97459821]\n",
      "W: [[2.00255159]], b: [0.99078794]\n",
      "W: [[2.00092535]], b: [0.99665921]\n",
      "W: [[2.00033558]], b: [0.99878845]\n",
      "W: [[2.0001217]], b: [0.99956063]\n",
      "W: [[2.00004413]], b: [0.99984066]\n",
      "W: [[2.00001601]], b: [0.99994221]\n",
      "W: [[2.0000058]], b: [0.99997904]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Training Data Set\n",
    "x_data = np.array([1,2,3,4,5]).reshape(5,1)\n",
    "t_data = np.array([3,5,7,9,11]).reshape(5,1)\n",
    "\n",
    "# 2. Weight, bias\n",
    "W = np.random.rand(1,1) # 2차 Matrix: [[0.32456]]\n",
    "b = np.random.rand(1)   # 스칼라\n",
    "\n",
    "# 3. Hypothesis - 모델 학습이 종료된 후 모델이 생성되고 나서, 예측값을 구할 때 사용\n",
    "def predict(x):\n",
    "    \n",
    "    y = np.dot(x, W) + b # y = X dot W + b\n",
    "    return y\n",
    "\n",
    "# 4. Loss Function: W와 b의 함수이므로 W와 b를 인자로 받아서 손실함수를 계산\n",
    "#                   그런데, 수치미분을 위해 W, b를 하나의 리스트 안에 넣어서 하나의 인자로 받음\n",
    "\n",
    "# 아래의 함수는 W, b가 한 개인 경우로 Hard Code했기 때문에 Simple Linear Regression에서만 사용 가능\n",
    "def loss_func(input_obj):\n",
    "    \n",
    "    # input_obj = [W, b]\n",
    "    input_W = input_obj[0]\n",
    "    input_b = input_obj[1]\n",
    "    \n",
    "    y = np.dot(x_data, input_W) + input_b     # Hypothesis\n",
    "    \n",
    "    return np.mean(np.power((t_data - y), 2)) # 손실함수\n",
    "\n",
    "# 5. 특정 W, b에서 손실함수를 편미분 -> 최적의 W, b?\n",
    "# 다변수 함수(W, b)에 대한 수치미분 코드 사용\n",
    "def numerical_derivative(f, x):\n",
    "    \n",
    "    # f: 편미분하려고 하는 다변수 함수 -> 손실함수\n",
    "    # x: 편미분하려고 하는 모든 값 -> W, b\n",
    "    # 따라서, [W, b]에 대해 각각 편미분이 진행\n",
    "    \n",
    "    delta_x = 1e-4\n",
    "    derivative_x = np.zeros_like(x) \n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'])\n",
    "    \n",
    "    while not it.finished: \n",
    "        idx = it.multi_index \n",
    "        # print('현재의 idx: {}'.format(idx)) \n",
    "\n",
    "        tmp = x[idx]                        \n",
    "        # print('현재의 tmp: {}'.format(tmp)) # 1.0 # 2.0\n",
    "        \n",
    "        # x에 대한 편미분\n",
    "        x[idx] = tmp + delta_x \n",
    "        fx_plus_delta = f(x) \n",
    "        \n",
    "        # 중앙차분 미분 준비\n",
    "        x[idx] = tmp - delta_x\n",
    "        fx_minus_delta = f(x) \n",
    "        \n",
    "        # 중앙차분\n",
    "        derivative_x[idx] = (fx_plus_delta - fx_minus_delta) / (2 * delta_x)\n",
    "        \n",
    "        # 두번째 독립변수에 대해 편미분 시행을 위해 원래 상태로 복구\n",
    "        x[idx] = tmp \n",
    "    \n",
    "        it.iternext() # 다음 iterator, 다음 칸으로 넘어감\n",
    "        \n",
    "    return derivative_x\n",
    "\n",
    "\n",
    "\n",
    "# 6. learning rate 설정\n",
    "learning_rate = 1e-4\n",
    "\n",
    "\n",
    "# 7. 반복 학습 진행: 손실함수 미분 -> W, b 갱신\n",
    "for step in range(300000):\n",
    "    \n",
    "    # 현재 W, b값을 입력 파라미터 값으로 저장하여 편미분 함수에 인자값으로 넘기기 위한 용도\n",
    "    # [W b]\n",
    "    input_param = np.concatenate((W.ravel(), b.ravel()), axis=0) # 1차원 벡터끼리 연결하기 위해 ravel 사용\n",
    "    \n",
    "    # 손실함수를 input_param에 대해 편미분 시행\n",
    "    derivative_result = learning_rate * numerical_derivative(loss_func, input_param)\n",
    "    \n",
    "    # W, b 갱신\n",
    "    # 위에서 합쳐진 인자를 각각 뽑아서 계산\n",
    "    W = W - derivative_result[:1].reshape(1, 1) # 2차원 - 2차원\n",
    "    b = b - derivative_result[1:] # 1차원 벡터\n",
    "    \n",
    "    if step%30000 == 0:\n",
    "        print('W: {}, b: {}'.format(W, b)) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [[2.]], b: [1.]\n",
      "[[19.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "\n",
    "# 1. Training Data Set\n",
    "x_data = np.array([1,2,3,4,5]).reshape(5,1)\n",
    "t_data = np.array([3,5,7,9,11]).reshape(5,1)\n",
    "\n",
    "# 2. Linear Regression Model 생성\n",
    "model = linear_model.LinearRegression()\n",
    "\n",
    "# 3. 학습 진행 -> 모델 생성\n",
    "model.fit(x_data, t_data)\n",
    "\n",
    "# 4. Weight, bias 출력\n",
    "print('W: {}, b: {}'.format(model.coef_, model.intercept_)) # W: [[2.]], b: [1.]\n",
    "\n",
    "# 5. Prediction\n",
    "print(model.predict([[9]])) # [[19.]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
