# Classification System

Training Data Setì˜ íŠ¹ì§•ê³¼ ë¶„í¬ë¥¼ ì´ìš©í•˜ì—¬ í•™ìŠµí•œ í›„, ë¯¸ì§€ì˜ ë°ì´í„°ì— ëŒ€í•´ ê²°ê³¼ê°€ ì–´ë–¤ ì¢…ë¥˜ì˜ ê°’ìœ¼ë¡œ ë¶„ë¥˜ë  ìˆ˜ ìˆëŠ” ì§€ ì˜ˆì¸¡í•˜ëŠ” ì‘ì—…

ë‹¤ì‹œ ë§í•´, 1ì— ê°€ê¹Œì›Œì§ˆ í™•ë¥ ê°’ì„ ì•Œì•„ë‚´ëŠ” ì‘ì—…ì´ë‹¤.

**[Classification Algorithm]**

- **Logistic Regression**
- KNN
- Naive Baise
- Decision Tree
- Random Forest

ìœ„ì˜ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ Linear Regression ì‘ì—…(=ê°’ì„ êµ¬í•˜ëŠ” ì‘ì—…)ì„ í•  ìˆ˜ë„ ìˆë‹¤.  





# Logistic Regression

ëŒ€í‘œì ì¸ ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ ì¤‘ ì •í™•ë„ê°€ ìƒë‹¹íˆ ë†’ì€ ê¸°ë²• ì¤‘ í•˜ë‚˜ë¡œ ë‚˜ì¤‘ì— ë”¥ëŸ¬ë‹ì˜ ê¸°ë³¸ Componentë¡œ ì‚¬ìš©ëœë‹¤.

Logistic Regressionì€ **Linear Regression ì§ì„ ì„ ê¸°ì¤€ìœ¼ë¡œ** ì–¼ë§ˆ ì •ë„ì˜ í™•ë¥ ë¡œ **ì–´ëŠ ì˜ì—­ì— ë“¤ì–´ê°€ëŠ” ì§€ íŒë‹¨**í•œë‹¤.

(Linear Regressionì€ ì§ì„  ìƒì˜ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ê¸°ë²•)

<details>
  <summary>ê·¸ë¦¼ìœ¼ë¡œ ì •ë¦¬</summary>
  <img src="md-images/logit.png">
</details>



## Scikit-learn

`pip3 install mglearn`: ë°ì´í„° ì…‹ì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•œ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ

- `x`: ndarray 2ì°¨ì› í‰ë©´ì˜ ë°ì´í„° í˜•íƒœë¡œ, xì¶• ì¢Œí‘œì™€ yì¶• ì¢Œí‘œë¥¼ ë‚˜íƒ€ëƒ„ (ì¢Œí‘œê°’)

- `y`: ì–´ë–¤ í˜•íƒœì˜ ì ì¸ ì§€ ë‚˜íƒ€ëƒ„

- `mglearn.discrete_scatter(xì¶•ì˜ ê°’, yì¶•ì˜ ê°’, ì ì˜ í˜•íƒœ)`

- 2ì°¨ì› ë°ì´í„°ë¡œ í•™ìŠµ

  xë°ì´í„°(ì…ë ¥)ì™€ yë°ì´í„°(ì •ë‹µ) ëª¨ë‘ 2ì°¨ì› í–‰ë ¬ ë°ì´í„°ë¡œ ë°”ê¿”ì¤˜ì•¼ í•¨

  ![image-20210302160606143](md-images/error.png)

### Simple Linear Regression

```python
## Training Data Set
x, y = mglearn.datasets.make_forge() # tuple
print(x) # xì¶• ì¢Œí‘œ, yì¶• ì¢Œí‘œ
print(y) # ì ì˜ í˜•íƒœ

# Visualization
mglearn.discrete_scatter(x[:,0], x[:,1], y)

## Linear Regression

# model
model = linear_model.LinearRegression()

# learning (2ì°¨ì›)
model.fit(x[:,0].reshape(-1,1), x[:,1].reshape(-1,1))

# ê²°ê³¼
print(model.coef_) # Weight: [[-0.17382295]]
print(model.intercept_) # bias: [4.5982984]

# Visualization (1ì°¨ì›)
mglearn.discrete_scatter(x[:,0], x[:,1], y)
plt.plot(x[:,0], x[:,0] * model.coef_.ravel() + model.intercept_) # (x, y) ê·¸ë˜í”„
plt.show()
```

<details>
  <summary>ë°ì´í„°ì˜ ë¶„í¬ íŒŒì•…</summary>
  <img src="md-images/data-line.png">
</details>



ğŸ™‹ğŸ»â€â™€ï¸ **ê·¸ëŸ¬ë©´ Linear Regression ì§ì„ ìœ¼ë¡œëŠ” ë¶„ë¥˜ë¥¼ í•  ìˆ˜ ì—†ë‚˜ìš”?**

ê³µë¶€ì‹œê°„ì— ë”°ë¥¸ í•©ê²© ì—¬ë¶€ë¥¼ íŒë‹¨í•´ë³´ì. (t_lable ìì²´ê°€ 0ë¶€í„° 1ë¡œ ë˜ì–´ìˆìœ¼ë¯€ë¡œ) í•´ê²°ëœë‹¤.

```python
import numpy as np
from sklearn import linear_model
import matplotlib.pyplot as plt

# Training Data Set
x_data = np.array([1, 2, 5, 8, 10]) # ê³µë¶€ì‹œê°„
t_data = np.array([0, 0, 0, 1, 1])  # í•©ê²©ì—¬ë¶€ (0: Fail, 1: Pass)

# sklearn
model = linear_model.LinearRegression()
model.fit(x_data.reshape(-1,1), t_data.reshape(-1,1))
print('ê¸°ìš¸ê¸°: {}, ì ˆí¸: {}'.format(model.coef_, model.intercept_))

# visualization
plt.scatter(x_data, t_data)
plt.plot(x_data, x_data * model.coef_.ravel() + model.intercept_)
plt.show()

# prediction
result = model.predict([[7]])
print(result) # [[0.63265306]] => 0.5ë³´ë‹¤ í¬ê¸° ë•Œë¬¸ì— ì‹œí—˜ì— í•©ê²©í•œë‹¤ê³  ê°„ì£¼

# ê·¸ëŸ¼ ì‹œí—˜ì— í†µê³¼í•˜ê¸° ìœ„í•œ ìµœì†Œ ì‹œê°„?
time = (0.5 - model.intercept_) / model.coef_.ravel()
print(time) # [5.97368421]
```

<details>
  <summary>ì§ì„  í˜•íƒœ</summary>
  <img src="md-images/lineimg.png">
</details>



ğŸ™‹ğŸ»â€â™€ï¸ **êµ³ì´ ì™œ Logistic Regressionì„ ì‚¬ìš©í•´ì•¼ í•˜ë‚˜ìš”?**

ê²°ì¸¡ì¹˜ì™€ ì´ìƒì¹˜ ì²˜ë¦¬?

ì‹¤ì œ ë°ì´í„°ì˜ ì˜ë¯¸ë¥¼ ì•Œ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— ë„ë©”ì¸ ê´€ë¦¬ìì™€ ìƒì˜í•œ í›„ ì´ìƒì¹˜ ë˜ëŠ” ì§€ëŒ€ì ì„ ì²˜ë¦¬í•´ì•¼ í•œë‹¤.

íŠ¸ë ˆì´ë‹ ë°ì´í„° ì…‹ì— ë”°ë¼ ì •í™•í•˜ì§€ ì•Šì€ ëª¨ë¸ì´ ë„ì¶œë˜ì–´ ì˜ˆì¸¡ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤. ê²°êµ­ Linear Regressionìœ¼ë¡œ ë¬¸ì œ í•´ê²° ì‹¤íŒ¨.

```python
import numpy as np
from sklearn import linear_model
import matplotlib.pyplot as plt

# Training Data Set
x_data = np.array([1, 2, 5, 8, 10, 30]) # ê³µë¶€ì‹œê°„
t_data = np.array([0, 0, 0, 1, 1, 1])  # í•©ê²©ì—¬ë¶€ (0: Fail, 1: Pass)

# sklearn
model = linear_model.LinearRegression()
model.fit(x_data.reshape(-1,1), t_data.reshape(-1,1))
print('ê¸°ìš¸ê¸°: {}, ì ˆí¸: {}'.format(model.coef_, model.intercept_))

# visualization
plt.scatter(x_data, t_data)
plt.plot(x_data, x_data * model.coef_.ravel() + model.intercept_)
plt.show()

# prediction
result = model.predict([[7]])
print(result) # [[0.41831972]] => 0.5ë³´ë‹¤ ì‘ê¸° ë•Œë¬¸ì— ì‹œí—˜ì— ë¶ˆí•©ê²©ìœ¼ë¡œ ê°„ì£¼

# ê·¸ëŸ¼ ì‹œí—˜ì— í†µê³¼í•˜ê¸° ìœ„í•œ ìµœì†Œ ì‹œê°„?
time = (0.5 - model.intercept_) / model.coef_.ravel()
print(time) # [9.33333333]
```

<details>
  <summary>ì§ì„  í˜•íƒœ</summary>
  <img src="md-images/line-img.png">
</details>

ğŸ¤·ğŸ»â€â™€ï¸ **ê·¸ëŸ¼ ì–´ë–»ê²Œ í•´ì•¼í•˜ì§€?**

ì§ì„ ì€ x ê°’ì´ ì»¤ì§€ë©´ t ê°’ë„ ì»¤ì§€ëŠ” ì›ë¦¬ì´ê¸° ë•Œë¬¸ì— ìœ„ì™€ ê°™ì€ ë¬¸ì œê°€ ë°œìƒí•˜ëŠ” ê²ƒì´ë‹¤.

ë‹¤ì‹œ ë§í•´, ì§ì„  ëª¨ë¸ ìì²´ê°€ 0ê³¼ 1ì‚¬ì´ì˜ ê°’ì„ ê°€ì ¸ì•¼ íŒë‹¨ ê¸°ì¤€ì ì¸ 0.5ë¡œ ì˜ˆì¸¡ì´ ê°€ëŠ¥í•˜ì§€ë§Œ, í° ê°’ì˜ ë°ì´í„°ê°€ ì¡´ì¬í•˜ê²Œ ë  ê²½ìš° í™•ë¥ ê°’ì´ ë„ì¶œë˜ì§€ ì•ŠëŠ” ë¬¸ì œê°€ ìƒê¸´ë‹¤.

ë”°ë¼ì„œ ì§ì„  ëª¨ë¸ì„ 0ê³¼ 1ì‚¬ì´ì˜ ê°’ì„ ê°€ì§€ëŠ” ê³¡ì„  ëª¨ë¸ë¡œ ë°”ê¿”ì¤˜ì•¼ í•œë‹¤.

ì¦‰ Linear Regression ì§ì„ : Wx + b ë¥¼ 0ê³¼ 1ì‚¬ì´ì˜ Sì ê³¡ì„ : sigmoid(Wx + b) ë¡œ ë³€í™˜ì‹œì¼œì•¼ í•œë‹¤.



## Simple Logistic Regression

ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ Linear Regression ì§ì„ ì„ Sì ê³¡ì„  í˜•íƒœì˜ ê·¸ë˜í”„ë¡œ ë³€í™˜

ê²°ê³¼ì ìœ¼ë¡œ, Linear Regression ëª¨ë¸(ì¶œë ¥): Wx+bê°€ ì–´ë– í•œ ê°’ì„ ê°€ì§€ë”ë¼ë„ ì¶œë ¥í•¨ìˆ˜ì¸ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ë©´ 0ê³¼ 1ì‚¬ì´ì˜ ì‹¤ìˆ˜ê°’ìœ¼ë¡œ ë„ì¶œëœë‹¤.

ì´ë•Œ ì‹¤ìˆ˜ê°’ì´ 0.5ì´ìƒì´ë©´ 1ì„ ì¶œë ¥í•˜ê³  0.5 ë¯¸ë§Œì´ë©´ 0ìœ¼ë¡œ ì¶œë ¥í•œë‹¤.

![image-20210302161211610](md-images/logic-formula.png)

```python
import numpy as np
import matplotlib.pyplot as plt

x_data = np.arange(-7, 8)
sigmoid_t_data = 1 / (1 + np.exp(-1 * x_data))

plt.plot(x_data, sigmoid_t_data)
plt.show()
```

<details>
  <summary>ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì˜ í˜•íƒœ</summary>
  <img src="md-images/sigmoid.png">
</details>



**Linear Regression**

ìµœì†Œì œê³±ë²•ì„ ì´ìš©í•˜ì—¬ ì†ì‹¤í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ê³  ê²½ì‚¬í•˜ê°•ë²•ì„ í†µí•´ ë°˜ë³µì ìœ¼ë¡œ ì†ì‹¤í•¨ìˆ˜ë¥¼ ë¯¸ë¶„í•˜ì—¬ W, bë¥¼ ê°±ì‹ í•˜ì—¬ ìµœì ì˜ W, bë¥¼ êµ¬í•œë‹¤.

* y: ë¡œì§€ìŠ¤í‹± ëª¨ë¸
* E: ì†ì‹¤í•¨ìˆ˜

![image-20210302161337661](md-images/linear-formula.png)

**Logistic Regression**

**ï¿®  ì‹œê·¸ëª¨ì´ë“œ ëª¨ë¸**

![image-20210302161355403](md-images/sigmoid-formula.png)

ê·¸ë˜í”„ì˜ ëª¨ì–‘ì´ Convex í˜•íƒœê°€ ì•„ë‹ˆê¸° ë•Œë¬¸ì— ê²½ì‚¬í•˜ê°•ë²•ì„ ì‚¬ìš©í•  ì‹œ, ì–´ëŠ ì§€ì ì—ì„œë‚˜ local minimaê°€ ë°œìƒí•  ìˆ˜ ìˆì–´ì„œ ìµœì ì˜ W, bê°’ì„ ì§€ë‹Œ global minimaì„ ì°¾ì„ ìˆ˜ê°€ ì—†ê²Œ ëœë‹¤.

ë”°ë¼ì„œ ìœ„ì˜ ì†ì‹¤í•¨ìˆ˜(Eë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤.

<details>
  <summary>Exponential í•¨ìˆ˜ ëª¨ì–‘</summary>
  <img src="md-images/expo-func.png">
</details>



**ï¿®  Cross Entropy(=Log Loss)**: Logistic Regressionì—ì„œ ì‚¬ìš©ë˜ëŠ” ì†ì‹¤í•¨ìˆ˜

Exponential í•¨ìˆ˜ë¥¼ Convex í•¨ìˆ˜ë¡œ ë³€í™˜ì‹œí‚¤ëŠ” ìˆ˜ì‹

![image-20210302161644202](md-images/log-loss.png)

<details>
  <summary>Logistic Regression ê³¼ì • ì„¤ëª…</summary>
  <p>
    Linear Regression ëª¨ë¸ì—ì„œ <b>ëª¨ë¸ê³¼ ì†ì‹¤í•¨ìˆ˜ë§Œ</b> ë°”ë€Œê³  ê³¼ì •ì€ Linear Regressionê³¼ ë™ì¼í•˜ë‹¤.
  </p>
  <img src="md-images/logit-exp.png">
</details>





## Simple Logistic Regression êµ¬í˜„

### Python

-----

ë‹¨ë³€ìˆ˜ì¼ ë•Œë§Œ ì‚¬ìš© ê°€ëŠ¥

- ìˆ˜ì¹˜ë¯¸ë¶„í•¨ìˆ˜
- `delta = 1e-7`: ë¡œê·¸ ì—°ì‚° ì‹œ, ë¬´í•œëŒ€ë¡œ ë°œì‚°í•˜ëŠ” ê²½ìš°ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ì•„ì£¼ ì‘ì€ ë”ë¯¸ ê°’ ì„¤ì •

```python
import numpy as np

# ìˆ˜ì¹˜ë¯¸ë¶„í•¨ìˆ˜
def numerical_derivative(f, x):
    # f: ì†ì‹¤í•¨ìˆ˜, x: ndarray => [W, b]
    
    delta_x = 1e-4
    derivative_x = np.zeros_like(x) 
    
    it = np.nditer(x, flags=['multi_index'])
    
    while not it.finished: 
        idx = it.multi_index 
        tmp = x[idx]                        

        x[idx] = tmp + delta_x 
        fx_plus_delta = f(x) 
        
        x[idx] = tmp - delta_x
        fx_minus_delta = f(x) 
        
        derivative_x[idx] = (fx_plus_delta - fx_minus_delta) / (2 * delta_x)
        
        x[idx] = tmp 
    
        it.iternext() 
        
    return derivative_x

# Training Data Set
x_data = np.arange(2,21, 2).reshape(-1,1) # ê³µë¶€ì‹œê°„(ë…ë¦½ë³€ìˆ˜): 2 ~ 20 ì§ìˆ˜
t_data = np.array([0,0,0,0,0,0,1,1,1,1]).reshape(-1,1)  # í•©ê²©ì—¬ë¶€(14ì‹œê°„ë¶€í„° í•©ê²©)

# W, b
W = np.random.rand(1,1) # í–‰ë ¬ê³± ì—°ì‚°ì„ ìœ„í•´ 2ì°¨ì› ë°°ì—´ë¡œ ìƒì„±
b = np.random.rand(1)   # braodcasting
    
# loss function
def loss_func(input_obj): #[W, b]
    input_W = input_obj[0]
    input_b = input_obj[1]
    
    # linear regression hypothesis
    z = np.dot(x_data, input_W) + input_b
    
    # logistic regression hypothesis
    y = 1 / (1 + np.exp(-1 * z))
    
    
    # cross entropy
    # ë¡œê·¸ ì—°ì‚° ì‹œ, ë¬´í•œëŒ€ë¡œ ë°œì‚°í•˜ëŠ” ê²½ìš°ë¥¼ ë°©ì§€í•˜ê¸°ìœ„í•´ delta ê°’ ì„¤ì •
    delta = 1e-7 # ì•„ì£¼ ì‘ì€ ë”ë¯¸ê°’
    
    log_loss = -np.sum(t_data*np.log(y+delta) + (1 - t_data)*np.log(1 - y+delta)) 
    
    return log_loss

# learning rate
learning_rate = 1e-4

# learning - Gradient Descent Algorithm ìˆ˜í–‰
for step in range(300000):
    # [W b]
    input_param = np.concatenate((W.ravel(), b), axis=0)
    
    # learning_rate * í¸ë¯¸ë¶„
    derivative_result = learning_rate * numerical_derivative(loss_func, input_param)
    
    # W, b ê°±ì‹ 
    W = W - derivative_result[0].reshape(-1, 1)
    b = b - derivative_result[1]
    
    if step % 30000 == 0:
        input_param = np.concatenate((W.ravel(), b), axis=0)
        print('W: {}, b: {}, loss: {}'.format(W.ravel(), b, loss_func(input_param)))

# Prediction - 13ì‹œê°„ ê³µë¶€í•  ê²½ìš°?
def logistic_predict(x): # x = [[13]]
    z = np.dot(x, W) + b
    y = 1 / (1 + np.exp(-1 * z))
    
    if y < 0.5:
        result = 0
    else:
        result = 1
        
    return result, y # (result: ê²°ê³¼ê°’, y: í™•ë¥ ê°’)

study_hour = np.array([[13]])
print(logistic_predict(study_hour)) # ê²°ê³¼: 1(í•©ê²©), í™•ë¥ : 0.54451492
```







### Tensorflow

-----

- `placeholder`: 1.x ë²„ì „ì—ì„œë§Œ ì¡´ì¬, 2.x ë²„ì „ì—ì„œëŠ” ì‚­ì œë¨

```python
import tensorflow as tf

# Training Data Set
x_data = np.arange(2,21, 2).reshape(-1,1)
t_data = np.array([0,0,0,0,0,0,1,1,1,1]).reshape(-1,1)

# placeholder
X = tf.placeholder(shape=[None,1], dtype=tf.float32)
T = tf.placeholder(shape=[None,1], dtype=tf.float32)

# W, b
W = tf.Variable(tf.random.normal([1,1]), name='weight')
b = tf.Variable(tf.random.normal([1]), name='bias')

# model
linear_model = tf.matmul(X, W) + b 
H = tf.sigmoid(linear_model) 

# log loss
loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=linear_model, labels=T))

# gradient descent algorithm
train = tf.train.GradientDescentOptimizer(learning_rate=1e-4).minimize(loss)

# learning
sess = tf.Session()
sess.run(tf.global_variables_initializer())

for step in range(300000):
    _, W_val, b_val, loss_val = sess.run([train, W, b, loss], feed_dict={X: x_data, T: t_data})
    
    if step % 30000 == 0:
        print('W: {}, b: {}, loss: {}'.format(W_val, b_val, loss_val))

# prediction
study_hour = np.array([[13]])
result = sess.run(H, feed_dict={X: study_hour})
print(result) # [[0.5790821]] í™•ë¥ ë¡œ í•©ê²©
```



### Scikit-learn

```python
from sklearn import linear_model

model = linear_model.LogisticRegression()
model.fit(x_data, t_data.ravel()) # 2ë²ˆì§¸ ì¸ì: 1ì°¨ì›

result = model.predict(study_hour)

result_proba = model.predict_proba(study_hour) # [[ë–¨ì–´ì§ˆ í™•ë¥ , í•©ê²©í•  í™•ë¥ ]]

print('W: {}, b: {}, í™•ë¥ : {}'.format(model.coef_, model.intercept_, result_proba))
print(result) # 0: ë¶ˆí•©ê²©
```



------

Reference: [ML_0302](https://github.com/sammitako/TIL/blob/master/Machine%20Learning/source-code/ML_0302.ipynb)

