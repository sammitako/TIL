# Tensorflow ì—°ìŠµ

### ì½”ë“œ ì´í•´

- `tf.constant`: ìƒìˆ˜ë…¸ë“œ ìƒì„± ì½”ë“œë¡œ ì‹¤ì œ ë¨¸ì‹ ëŸ¬ë‹ êµ¬í˜„ ì‹œì—ëŠ” ìì£¼ ì‚¬ìš©ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤.

- `tf.float32`: ë°ì´í„° íƒ€ì… ì¤‘ tensorflowì˜ ì‹¤ìˆ˜ëŠ” 32ë¹„íŠ¸ê°€ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©ëœë‹¤.

- `tf.placeholder`: ë…¸ë“œë¥¼ ì‹¤í–‰ì‹œí‚¤ëŠ” ì‹œì ì— graphì— ê°’ì„ ì£¼ì… (nodeì˜ ê°’ì„ ì„¤ì •)

  (ì…ë ¥ê°’ì„ "ë°›ì•„ì£¼ëŠ”" ì…ë ¥ íŒŒë¼ë¯¸í„° ë…¸ë“œ)

  1. Training Data Setì„ ë°›ì•„ë“¤ì—¬ì„œ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ìš©ë„
  2. ëª¨ë¸ì´ ì™„ì„±ëœ í›„, ë¯¸ì§€ì˜ xê°’ì„ ëª¨ë¸ì— ë„£ì–´ ì˜ˆì¸¡ê°’ì„ ë„ì¶œí•˜ê¸° ìœ„í•œ ìš©ë„

- `feed_dict`: ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°ì´í„°ë¥¼ `placeholder`ì— ì „ë‹¬




```python
import tensorflow as tf

# ìƒìˆ˜ë…¸ë“œ ìƒì„± (ìì£¼ ì‚¬ìš© ì•ˆë¨)
node1 = tf.constant(10, dtype=tf.float32) 
node2 = tf.constant(10, dtype=tf.float32) 

# ìˆ˜ì¹˜ì—°ì‚°
node3 = node1 + node2 

# ê·¸ë˜í”„ ì‹¤í–‰
sess = tf.Session() # 2.xë²„ì „ì—ì„œëŠ” sessionì´ ì‚­ì œë¨
print(sess.run(node3)) # 30.0

# Data Flow Graphì— ì…ë ¥ê°’ í• ë‹¹ ì‹œ, placeholderë¥¼ ì´ìš©
import tensorflow as tf

node1 = tf.placeholder(dtype=tf.float32) # ì…ë ¥ íŒŒë¼ë¯¸í„°ë¥¼ "ë°›ì•„ì£¼ëŠ”" ë°”êµ¬ë‹ˆ
node2 = tf.placeholder(dtype=tf.float32)
node3 = node1 + node2

sess = tf.Session()
result = sess.run(node3, feed_dict={node1: 10, node2: 20})
print(result) # 30.0
```





# Simple Linear Regression

ğŸ‘‰ğŸ¼ **epoch ìˆ˜?**

í•™ìŠµ **ë°ì´í„°ì˜ ì–‘**, íŠ¹ì„± ë˜ëŠ” Tensorflow ê·¸ë˜í”„ ìƒíƒœì— ë”°ë¼ ì¡°ì ˆ

epoch ìˆ˜ë¥¼ ì ì •ëŸ‰ë³´ë‹¤ í¬ê²Œ ì¡ì„ ê²½ìš°, overfitting í˜„ìƒì´ ë°œìƒí•˜ê³  ì ê²Œ ì¡ì„ ê²½ìš° underfitting í˜„ìƒì´ ë°œìƒí•œë‹¤.

## 1. Tensorflow

ì‹¤ì œë¡œ í•™ìŠµì´ ì§„í–‰ë  ìˆ˜ë¡ ì†ì‹¤í•¨ìˆ˜ ê°’ì´ 0ê³¼ ê°€ê¹Œì›Œ ì§€ëŠ” ì§€ë¥¼ í™•ì¸í•˜ì—¬ í•™ìŠµì´ ì œëŒ€ë¡œ ì§„í–‰ë˜ëŠ” ì§€ ê°„ì ‘ì ìœ¼ë¡œ íŒë‹¨í•´ì•¼ í•œë‹¤.

ëª¨ë¸ì˜ ì •í™•ë„ì— ëŒ€í•œ íŒë‹¨ì€ ì •í™•ë„ ì¸¡ì •(accuracy, recall, precision)ì„ í†µí•´ ê°€ëŠ¥í•˜ë‹¤.


### ì½”ë“œ ì´í•´

- `tf.Variable`: ê°’ì´ ë³€í•˜ëŠ” ë…¸ë“œë¥¼ ì§€ì¹­

- `tf.random.normal(shape)`: 0ê³¼ 1ì‚¬ì´ì˜ í‘œì¤€ì •ê·œë¶„í¬ì—ì„œ ì‹¤ìˆ˜ ë‚œìˆ˜ ê°’ì„ ë°œìƒ

- `tf.placeholder(shape, dtype)`ë¥¼ ì‚¬ìš©ì‹œ, ì…ë ¥ ë°ì´í„°ê°€ 2ì°¨ì› ì´ìƒì¼ ê²½ìš° ì°¨ì›ì„ ëª…ì‹œí•´ì¤˜ì•¼ í•œë‹¤.

  ê·¸ëŸ°ë°, ê·¸ë˜í”„ ìˆ˜í–‰ì´ ëë‚˜ì„œ Predict Modelì´ ë§Œë“¤ì–´ì§€ê³  ì‚¬ìš©ìê°€ ì…ë ¥ ë°ì´í„° xë¥¼ ëª¨ë¸ì— ì£¼ì…í•˜ê²Œ ë  ë•Œë¥¼ ê³ ë ¤í•˜ì—¬ `shape=[None, 1]`ìœ¼ë¡œ ì¨ì¤˜ì•¼ í•œë‹¤.

  ì´ë•Œ `1`ì€ ì»¬ëŸ¼ì˜ ìˆ˜(ì˜ˆë¥¼ ë“¤ì–´, ê³µë¶€ì‹œê°„ ë˜ëŠ” ì‹œí—˜ì„±ì )ì„ ì˜ë¯¸í•˜ê³  ë³€í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì—´ì˜ ìˆ˜ëŠ” ë°˜ë“œì‹œ ëª…ì‹œë¥¼ í•´ì£¼ì–´ì•¼ í•œë‹¤.

  ì¦‰, Simple Linear Regressionì€ ë…ë¦½ë³€ìˆ˜ê°€ í•œ ê°œì´ê¸° ë•Œë¬¸ì— `1`ì¸ ê²ƒì´ë‹¤.

  ë°˜ë©´, `None`ì€ placeholderê°€ ëª‡ ê°œì˜ ë°ì´í„°(ë ˆì½”ë“œ)ê°€ ë“¤ì–´ì˜¬ ì§€ëŠ” ìƒê´€í•˜ì§€ ì•ŠëŠ”ë‹¤ë¥¼ ì˜ë¯¸í•œë‹¤.

  ë”°ë¼ì„œ `shape=[None, 1]`: 2ì°¨ì› Matrixì´ê³ , ì»¬ëŸ¼ì˜ ê°¯ìˆ˜ê°€ í•œ ê°œ, ëŒ€ì‹  ë ˆì½”ë“œì˜ ìˆ˜ëŠ” ìƒê´€ ì—†ìŒ

- `tf.matmul()`: í–‰ë ¬ê³± ì—°ì‚° í•¨ìˆ˜

- `tf.reduce_mean()`: ë‹¤ì°¨ì› í–‰ë ¬ì˜ í‰ê· ì„ êµ¬í•˜ëŠ” í•¨ìˆ˜ë¡œ defultëŠ” ëª¨ë“  ì›ì†Œì˜ í‰ê· ì´ë©° axisë¥¼ ì§€ì •í•˜ë©´ í–‰ì˜ í‰ê· , ì—´ì˜ í‰ê· ì„ ë¦¬í„´í•œë‹¤.

- `tf.Session().run(tf.global_variables_initializer())`

  : Tensorflowì˜ ë³€ìˆ˜ë¥¼ ë‚´ë¶€ì ìœ¼ë¡œ ì‚¬ìš© ì‹œ, **ë°˜ë“œì‹œ** ì´ˆê¸°í™” ì‘ì—…ì„ ì§„í–‰í•´ì¤˜ì•¼ í•œë‹¤.

  (2.x ë²„ì „ì—ì„œëŠ” ì‚­ì œë¨)

- `_`: ë¦¬í„´ê°’ ì‚¬ìš© ì•ˆí•¨


```python
import numpy as np      
import pandas as pd      
import tensorflow as tf 

# 1. Training Data Set
x_data = (np.array([1,2,3,4,5])).reshape(5,1)  
t_data = (np.array([3,5,7,9,11])).reshape(5,1) 
print(x_data)

# 2. Placeholder
X = tf.placeholder(shape=[None,1], dtype=tf.float32) 
T = tf.placeholder(shape=[None,1], dtype=tf.float32)

# 3. Hypothesis

# Weight ,bias 
W = tf.Variable(tf.random.normal([1,1]), name='weight')
b = tf.Variable(tf.random.normal([1]), name='bias')  

# Predict Model
H = tf.matmul(X, W) + b 
    
# 4. Loss Function
loss = tf.reduce_mean(tf.square(H - T)) # í–‰ë ¬ì°¨ì— ëŒ€í•´ ê°ê°ì˜ ê°’ì„ ì œê³± í›„ í‰ê· êµ¬í•¨

# 5. train
train = tf.train.GradientDescentOptimizer(learning_rate=1e-3).minimize(loss) 

# 6. Session ë° ì „ì—­ë³€ìˆ˜ ì´ˆê¸°í™”
sess = tf.Session() 
sess.run(tf.global_variables_initializer()) 

 
# 7. Learning
for step in range(30000): 
    _, W_val, b_val, loss_val  = sess.run([train, W, b, loss], feed_dict={X: x_data, T: t_data}) 
    
    if step%3000 == 0: 
        print('W: {}, b: {}, loss: {}'.format(W_val, b_val, loss_val))
        
        
# 8. Prediction
result = sess.run(H, feed_dict={X: [[9]]}) 
print('ì˜ˆì¸¡ê°’: {}'.format(result)) # 18.999674
```

<details>
  <summary>#5 ê¹Œì§€ì˜ Tensorflow ê·¸ë˜í”„ í˜•íƒœ</summary>
  <img src="md-images/graph_tensor.png">
</details>

-----

**[ìŠ¤í… ë³„ ì •ë¦¬]**

1. **Training Data Set**

   - ì…ë ¥ ë°ì´í„°
   - 1ì°¨ì› ë²¡í„° â†’ 2ì°¨ì› í–‰ë ¬(matrix)

   ```python
   import numpy as np       # ìë£Œêµ¬ì¡°
   import pandas as pd      # ë°ì´í„°í•¸ë“¤ë§
   import tensorflow as tf  # ë¨¸ì‹ ëŸ¬ë‹ 
   
   # Training Data Set
   x_data = (np.array([1,2,3,4,5])).reshape(5,1)  # ë…ë¦½ë³€ìˆ˜ 1ê°œ
   t_data = (np.array([3,5,7,9,11])).reshape(5,1) # lable
   print(x_data)
   ```

2. **Placeholder**

   1. Data â†’ Data Flow Graph
   2. ì˜ˆì¸¡ ëª¨ë¸ì´ ì™„ì„±ëœ í›„ì—ëŠ” ë¯¸ì§€ì˜ Xë¥¼ ì…ë ¥í•˜ì—¬ ì˜ˆì¸¡ê°’ì„ ë„ì¶œí•˜ëŠ” ìš©ë„ë¡œ ì‚¬ìš©

   ```python
   # Placeholder
   X = tf.placeholder(shape=[None,1], dtype=tf.float32) 
   T = tf.placeholder(shape=[None,1], dtype=tf.float32)
   ```

3. **Hypothesis**

   ì¼ì°¨í•¨ìˆ˜: y = Wx + b

   - **Weight, bias ì„¤ì •**

     - ëœë¤í•œ ì´ˆê¸°ê°’
     - ë‚˜ì¤‘ì— ê·¸ë˜í”„ê°€ ë°˜ë³µì ìœ¼ë¡œ ìˆ˜í–‰ë˜ë©´ì„œ ê°’ì´ ê³„ì† ê°±ì‹ ë¨

   - **Predict Model ì •ì˜**

     ì¼ì°¨í•¨ìˆ˜ë¥¼ í–‰ë ¬ì‹ìœ¼ë¡œ ë³€í™˜: y=Xâ€¢W+b

     - X, W: 2ì°¨ì› í–‰ë ¬
     - b: ìŠ¤ì¹¼ë¼ (broadcasting)
     - y: ì˜ˆì¸¡ê°’ (=prediction model)

   ```python
   # Weight ,bias ì„¤ì • - ê³„ì† ë³€í•˜ëŠ” ê°’
   # Simple Linear Regressionì´ë¯€ë¡œ ë…ë¦½ë³€ìˆ˜ê°€ í•œê°œë‹ˆê¹Œ Wë„ í•œ ê°œë§Œ êµ¬í•˜ë©´ ëœë‹¤.
   # [1,1]: 2ì°¨ì› Matrix í˜•íƒœë¡œ W "í•œê°œ" êµ¬í•¨
   W = tf.Variable(tf.random.normal([1,1]), name='weight') 
                                                           
   
   # í–‰ë ¬ê³± ì—°ì‚°ì„ ì•ˆí•˜ë¯€ë¡œ êµ³ì´ 2ì°¨ì› Matrix í˜•íƒœë¡œ êµ¬í•  í•„ìš” ì—†ìŒ
   # [1]: 1ì°¨ì› ë²¡í„° í˜•íƒœë¡œ W "í•œê°œ" êµ¬í•¨
   b = tf.Variable(tf.random.normal([1]), name='bias') 
   
   # Predict Model ì •ì˜ 
   H = tf.matmul(X, W) + b # y = Wx + b => 2ì°¨ì› í–‰ë ¬ë¡œ ì²˜ë¦¬ => y = X dot W + b
   ```

4. **Loss Function**

   - W,bë¥¼ êµ¬í•˜ê¸° ìœ„í•´ í‰ê· ì œê³±ì˜¤ì°¨ë¥¼ ì´ìš©í•œ ìµœì†Œì œê³±ë²•ì„ í†µí•´ ì†ì‹¤í•¨ìˆ˜ ì •ì˜
   - í–‰ë ¬ì°¨ì— ëŒ€í•´ ê°ê°ì˜ ê°’ì„ ì œê³± í›„ í‰ê· êµ¬í•¨

   ```python
   # Loss Function
   loss = tf.reduce_mean(tf.square(H - T))
   ```

5. **train ë…¸ë“œ ìƒì„±**

   ì†ì‹¤í•¨ìˆ˜ë¥¼ ë¯¸ë¶„í•´ë‚˜ê°€ë©´ì„œ W, bë¥¼ ê°±ì‹ 

   - ê²½ì‚¬í•˜ê°•ë²• ì•Œê³ ë¦¬ì¦˜ì„ ìˆ˜í–‰í•  í•™ìŠµìš© ë…¸ë“œ ìƒì„±í•˜ì—¬ (ë¯¸ë¶„ì„ í†µí•´) ì†ì‹¤í•¨ìˆ˜ì˜ ê°’ì„ ì¤„ì—¬ê°€ë©´ì„œ W, bì˜ ê°’ì„ 1ë²ˆ ê°±ì‹ 
   - ìœ„ì˜ ì‘ì—…ì„ ë°˜ë³µ ìˆ˜í–‰í•˜ì—¬ ìµœì ì˜ W, bë¥¼ êµ¬í•¨

   ```python
   # train
   train = tf.train.GradientDescentOptimizer(learning_rate=1e-3).minimize(loss)
   ```

6. **Session, ì „ì—­ë³€ìˆ˜ ì´ˆê¸°í™”**

   - Tensorflow ê·¸ë˜í”„ë¥¼ ì‹¤í–‰
   - 2.x ë²„ì „ì—ì„œëŠ” ë¶ˆí•„ìš”

   ```python
   # Session, ì´ˆê¸°í™”
   sess = tf.Session() 
   sess.run(tf.global_variables_initializer()) 
   ```

7. **ë°˜ë³µ í•™ìŠµ (Learning)**

   - 1 epoch: Training Data Set ì „ì²´ë¥¼ ì´ìš©í•˜ì—¬ í•œ ë²ˆ í•™ìŠµ
   - í•™ìŠµìš© - train ë…¸ë“œë§Œ ì‹¤í–‰í•˜ë©´ ëª¨ë“  í•˜ìœ„ë…¸ë“œë“¤ì´ ì‹¤í–‰ë¨
   - í™•ì¸ìš© - êµ¬í•´ì•¼í•˜ëŠ” ê°’: W, b ê·¸ë¦¬ê³  ì†ì‹¤í•¨ìˆ˜ ê°’: 0ê³¼ ê°€ê¹Œì›Œì§ˆ ë§Œí¼ ì‘ì•„ì ¸ì•¼í•¨

   ```python
   # Learning
   for step in range(30000): # 3000 epoch
       _, W_val, b_val, loss_val  = sess.run([train, W, b, loss], feed_dict={X: x_data, T: t_data}) 
       
       if step%3000 == 0: # 10ë²ˆë§Œ ì¶œë ¥
           # y = 2x + 1, ë”°ë¼ì„œ W = 2, b = 1
           print('W: {}, b: {}, loss: {}'.format(W_val, b_val, loss_val))
   ```

8. **ì˜ˆì¸¡ (Prediction)**

   í•™ìŠµ ì¢…ë£Œ í›„ ìµœì ì˜ W, bê°€ ê³„ì‚°ë˜ë©´ ì´ë¥¼ ì´ìš©í•˜ì—¬ ëª¨ë¸(H)ì´ ì™„ì„±ë¨

   ```python
   # Prediction(ì˜ˆì¸¡) ì‘ì—…
   
   result = sess.run(H, feed_dict={X: [[9]]}) # X: 2ì°¨ì› Matrix í˜•íƒœì´ë¯€ë¡œ ì¤‘ì²© ë¦¬ìŠ¤íŠ¸(1í–‰1ì—´)ë¡œ êµ¬í˜„
   print('ì˜ˆì¸¡ê°’: {}'.format(result)) # 18.999674
   ```

## 2. Python

### ì½”ë“œ ì´í•´

- ì†ì‹¤í•¨ìˆ˜: W, bì˜ í•¨ìˆ˜

  ë”°ë¼ì„œ W, bë¥¼ ì¸ìë¡œ ë°›ì•„ì„œ ì†ì‹¤í•¨ìˆ˜ë¥¼ ê³„ì‚°í•´ì•¼ í•œë‹¤. ì†ì‹¤í•¨ìˆ˜ë¥¼ ë¯¸ë¶„í•  ë•Œ ì´ ì¸ìë“¤ì„ ìˆ˜ì¹˜ë¯¸ë¶„ í•¨ìˆ˜ì˜ ì¸ìë¡œ ë„˜ê²¨ì£¼ì–´ì•¼ í•˜ëŠ”ë°, ë§Œì•½ ì¸ìë¥¼ ê°ê° ë”°ë¡œ ì¨ì£¼ê²Œ ë˜ë©´ ìˆ˜ì¹˜ë¯¸ë¶„ í•¨ìˆ˜ ë‚´ì—ì„œ ì†ì‹¤í•¨ìˆ˜ë¥¼ ì²˜ë¦¬í•˜ê¸°ê°€ í˜ë“¤ê²Œ ëœë‹¤.

  ê·¸ë˜ì„œ ì†ì‹¤í•¨ìˆ˜ì˜ ì¸ìê°’ì„ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ì£¼ì§€ë§Œ ê·¸ ì¸ìê°’ì„ ì†ì‹¤í•¨ìˆ˜ ë‚´ì—ì„œëŠ” 2ê°œì˜ ê°’ìœ¼ë¡œ ë”°ë¡œ ì‚¬ìš©í•œë‹¤.

- `ravel()`: 2ì°¨ì› Matrix â†’ 1ì°¨ì› ë²¡í„°ë¡œ ë³€ê²½

- `numpy.concatenate(axis=0)`: ê°€ë¡œ ë°©í–¥ìœ¼ë¡œ ê²°í•©

- `[:1]`: ì²« ë²ˆì§¸ ìš”ì†Œë¥¼ ë²¡í„°ë¡œ ë½‘ì•„ëƒ„

  ì´ë•Œ `[:1]`ëŒ€ì‹  `[0]`ì„ í•´ì£¼ë©´ ìŠ¤ì¹¼ë¼ ê°’ì´ ë‚˜ì˜¤ê¸° ë•Œë¬¸ì— ë²¡í„°ì— ëŒ€í•´ì„œë§Œ ì“¸ ìˆ˜ ìˆëŠ” `reshape()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì§€ ëª»í•˜ê²Œ ëœë‹¤.

  ì¦‰, ì—°ì‚°ì„ ìœ„í•´ slicingì„ í†µí•´ ndarray í˜•íƒœë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•¨ì´ë‹¤.



**[Python ì½”ë“œ êµ¬í˜„ì‹œ ì£¼ì˜í•´ì•¼í•  ì‚¬í•­]**

- í¸ë¯¸ë¶„ì„ í†µí•´ W, b ê°±ì‹ 
- ë°ì´í„° êµ¬ì¡°(shape) ì²˜ë¦¬



```python
import numpy as np

# 1. Training Data Set
x_data = np.array([1,2,3,4,5]).reshape(5,1)
t_data = np.array([3,5,7,9,11]).reshape(5,1)

# 2. Weight, bias
W = np.random.rand(1,1) 
b = np.random.rand(1)   

# 3. Hypothesis 
def predict(x):
    
    y = np.dot(x, W) + b 
    return y

# 4. Loss Function
def loss_func(input_obj):
    
    # input_obj = [W, b]
    input_W = input_obj[0]
    input_b = input_obj[1]
    
    y = np.dot(x_data, input_W) + input_b     
    
    return np.mean(np.power((t_data - y), 2)) 

# 5. ë‹¤ë³€ìˆ˜ í•¨ìˆ˜(W, b)ì— ëŒ€í•œ ìˆ˜ì¹˜ë¯¸ë¶„
def numerical_derivative(f, x):
    
    # f: í¸ë¯¸ë¶„í•˜ë ¤ê³  í•˜ëŠ” ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ -> ì†ì‹¤í•¨ìˆ˜
    # x: í¸ë¯¸ë¶„í•˜ë ¤ê³  í•˜ëŠ” ëª¨ë“  ê°’ -> W, b
    # ë”°ë¼ì„œ, [W, b]ì— ëŒ€í•´ ê°ê° í¸ë¯¸ë¶„ì´ ì§„í–‰
    
    delta_x = 1e-4
    derivative_x = np.zeros_like(x) 
    
    it = np.nditer(x, flags=['multi_index'])
    
    while not it.finished: 
        idx = it.multi_index 
        print('í˜„ì¬ì˜ idx: {}'.format(idx)) 

        tmp = x[idx]                        
        print('í˜„ì¬ì˜ tmp: {}'.format(tmp)) # 1.0 # 2.0
        
        # xì— ëŒ€í•œ í¸ë¯¸ë¶„
        x[idx] = tmp + delta_x 
        fx_plus_delta = f(x) 
        
        # ì¤‘ì•™ì°¨ë¶„ ë¯¸ë¶„ ì¤€ë¹„
        x[idx] = tmp - delta_x
        fx_minus_delta = f(x) 
        
        # ì¤‘ì•™ì°¨ë¶„
        derivative_x[idx] = (fx_plus_delta - fx_minus_delta) / (2 * delta_x)
        
        # ë‘ë²ˆì§¸ ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•´ í¸ë¯¸ë¶„ ì‹œí–‰ì„ ìœ„í•´ ì›ë˜ ìƒíƒœë¡œ ë³µêµ¬
        x[idx] = tmp 
    
        it.iternext() # ë‹¤ìŒ iterator, ë‹¤ìŒ ì¹¸ìœ¼ë¡œ ë„˜ì–´ê°
        
    return derivative_x

# 6. learning rate
learning_rate = 1e-4

# 7. Learning
for step in range(300000):
		# í˜„ì¬ W, bê°’ì„ ì…ë ¥ íŒŒë¼ë¯¸í„° ê°’ìœ¼ë¡œ ì €ì¥í•˜ì—¬ í¸ë¯¸ë¶„ í•¨ìˆ˜ì— ì¸ìê°’ìœ¼ë¡œ ë„˜ê¸°ê¸° ìœ„í•œ ìš©ë„: [W b]
    input_param = np.concatenate((W.ravel(), b.ravel()), axis=0) 
    
    # ì†ì‹¤í•¨ìˆ˜ë¥¼ input_paramì— ëŒ€í•´ í¸ë¯¸ë¶„ ì‹œí–‰
    derivative_result = learning_rate * numerical_derivative(loss_func, input_param)
    
    # W, b ê°±ì‹ 
    W = W - derivative_result[:1].reshape(1.1) 
    b = b - derivative_result[1:] 
    
    if step % 30000 == 0:
        print('W: {}, b: {}'.format(W, b)) # W: [[2.0000058]], b: [0.99997904]
```



**[ìŠ¤í… ë³„ ì •ë¦¬]**

1. **Training Data Set**

   ```python
   # Training Data Set
   x_data = np.array([1,2,3,4,5]).reshape(5,1)
   t_data = np.array([3,5,7,9,11]).reshape(5,1)
   ```

2. **Weight, bias**

   ```python
   # Weight, bias
   W = np.random.rand(1,1) # 2ì°¨ Matrix: [[0.32456]]
   b = np.random.rand(1)   # ìŠ¤ì¹¼ë¼
   ```

3. **Hypothesis**

   ëª¨ë¸ í•™ìŠµì´ ì¢…ë£Œëœ í›„ ëª¨ë¸ì´ ìƒì„±ë˜ê³  ë‚˜ì„œ, ì˜ˆì¸¡ê°’ì„ êµ¬í•  ë•Œ ì‚¬ìš©

   ```python
   # Hypothesis
   def predict(x):
       
       y = np.dot(x, W) + b # y = X dot W + b
       return y
   ```

   

4. **Loss Function**

   - Wì™€ bì˜ í•¨ìˆ˜ì´ë¯€ë¡œ Wì™€ bë¥¼ ì¸ìë¡œ ë°›ì•„ì„œ ì†ì‹¤í•¨ìˆ˜ë¥¼ ê³„ì‚°
   - ê·¸ëŸ°ë°, ìˆ˜ì¹˜ë¯¸ë¶„ì„ ìœ„í•´ W, bë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ ì•ˆì— ë„£ì–´ì„œ í•˜ë‚˜ì˜ ì¸ì: `(input_obj)`ë¡œ ë°›ìŒ
   - ë”°ë¼ì„œ ì•„ë˜ì˜ í•¨ìˆ˜ëŠ” W, bê°€ í•œ ê°œì¸ ê²½ìš°ë¡œ Hard Codeí–ˆê¸° ë•Œë¬¸ì— Simple Linear Regressionì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥

   ```python
   # ì†ì‹¤í•¨ìˆ˜
   def loss_func(input_obj):
       
       # input_obj = [W, b]
       input_W = input_obj[0]
       input_b = input_obj[1]
       
       y = np.dot(x_data, input_W) + input_b     # Hypothesis
       
       return np.mean(np.power((t_data - y), 2)) # ì†ì‹¤í•¨ìˆ˜
   ```

5. **ë‹¤ë³€ìˆ˜ í•¨ìˆ˜(W, b)ì— ëŒ€í•œ ìˆ˜ì¹˜ë¯¸ë¶„**

   íŠ¹ì • W, bì—ì„œ ì†ì‹¤í•¨ìˆ˜ë¥¼ í¸ë¯¸ë¶„ -> ìµœì ì˜ W, b?

   **ì¸ìê°’**

   - f: í¸ë¯¸ë¶„í•˜ë ¤ê³  í•˜ëŠ” ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ -> ì†ì‹¤í•¨ìˆ˜

   - x: í¸ë¯¸ë¶„í•˜ë ¤ê³  í•˜ëŠ” ëª¨ë“  ê°’ -> W, b

     ë”°ë¼ì„œ, [W, b]ì— ëŒ€í•´ ê°ê° í¸ë¯¸ë¶„ì´ ì§„í–‰

   ```python
   # ë‹¤ë³€ìˆ˜í•¨ìˆ˜ì— ëŒ€í•œ ìˆ˜ì¹˜ë¯¸ë¶„ ì½”ë“œ
   def numerical_derivative(f, x):
       
       delta_x = 1e-4
       derivative_x = np.zeros_like(x) 
       
       it = np.nditer(x, flags=['multi_index'])
       
       while not it.finished: 
           idx = it.multi_index 
           print('í˜„ì¬ì˜ idx: {}'.format(idx)) 
   
           tmp = x[idx]                        
           print('í˜„ì¬ì˜ tmp: {}'.format(tmp)) # 1.0 # 2.0
           
           # xì— ëŒ€í•œ í¸ë¯¸ë¶„
           x[idx] = tmp + delta_x 
           fx_plus_delta = f(x) 
           
           # ì¤‘ì•™ì°¨ë¶„ ë¯¸ë¶„ ì¤€ë¹„
           x[idx] = tmp - delta_x
           fx_minus_delta = f(x) 
           
           # ì¤‘ì•™ì°¨ë¶„
           derivative_x[idx] = (fx_plus_delta - fx_minus_delta) / (2 * delta_x)
           
           # ë‘ë²ˆì§¸ ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•´ í¸ë¯¸ë¶„ ì‹œí–‰ì„ ìœ„í•´ ì›ë˜ ìƒíƒœë¡œ ë³µêµ¬
           x[idx] = tmp 
       
           it.iternext() # ë‹¤ìŒ iterator, ë‹¤ìŒ ì¹¸ìœ¼ë¡œ ë„˜ì–´ê°
           
       return derivative_x
   ```

6. **Learning Rate ì„¤ì •**

   ```python
   # learning rate
   learning_rate = 1e-4
   ```

7. **ë°˜ë³µ í•™ìŠµ**

   ì†ì‹¤í•¨ìˆ˜ ë¯¸ë¶„ -> W, b ê°±ì‹ 

   ```python
   for step in range(300000):
       
       # í˜„ì¬ W, bê°’ì„ ì…ë ¥ íŒŒë¼ë¯¸í„° ê°’ìœ¼ë¡œ ì €ì¥í•˜ì—¬ í¸ë¯¸ë¶„ í•¨ìˆ˜ì— ì¸ìê°’ìœ¼ë¡œ ë„˜ê¸°ê¸° ìœ„í•œ ìš©ë„
       # [W b]
       input_param = np.concatenate((W.ravel(), b.ravel()), axis=0) # 1ì°¨ì› ë²¡í„°ë¼ë¦¬ ì—°ê²°í•˜ê¸° ìœ„í•´ ravel ì‚¬ìš©
       
       # ì†ì‹¤í•¨ìˆ˜ë¥¼ input_paramì— ëŒ€í•´ í¸ë¯¸ë¶„ ì‹œí–‰
       derivative_result = learning_rate * numerical_derivative(loss_func, input_param)
       
       # W, b ê°±ì‹ 
       # ìœ„ì—ì„œ í•©ì³ì§„ ì¸ìë¥¼ ê°ê° ë½‘ì•„ì„œ ê³„ì‚°
       W = W - derivative_result[:1].reshape(1, 1) # 2ì°¨ì› - 2ì°¨ì›
       b = b - derivative_result[1:] # 1ì°¨ì› ë²¡í„°
       
       if step % 30000 == 0:
           print('W: {}, b: {}'.format(W, b)) # W: [[2.0000058]], b: [0.99997904]
   ```

## 3. Sklearn

### ì„¤ì •

- `conda avtivate data_env`
- `pip install sklearn`

### ì½”ë“œ ì´í•´

- `model.coef_`: ì™„ì„±ëœ ëª¨ë¸ì— ëŒ€í•œ Wê°’
- `model.intercept_`: ì™„ì„±ëœ ëª¨ë¸ì— ëŒ€í•œ bê°’

```python
import numpy as np
from sklearn import linear_model

# 1. Training Data Set
x_data = np.array([1,2,3,4,5]).reshape(5,1)
t_data = np.array([3,5,7,9,11]).reshape(5,1)

# 2. Model
model = linear_model.LinearRegression()

# 3. Learning
model.fit(x_data, t_data)
# Weight, bias ì¶œë ¥
print('W: {}, b: {}'.format(model.coef_, model.intercept_)) # W: [[2.]], b: [1.]

# 4. Prediction
print(model.predict([[9]])) # [[19.]]
```



**[ìŠ¤í… ë³„ ì •ë¦¬]**

1. **Training Dat Set**

   ```python
   import numpy as np
   from sklearn import linear_model
   
   # Training Data Set
   x_data = np.array([1,2,3,4,5]).reshape(5,1)
   t_data = np.array([3,5,7,9,11]).reshape(5,1)
   ```

2. **Model**

   Linear Regression Model ìƒì„±

   ```python
   # Model
   model = linear_model.LinearRegression()
   ```

3. **Learning**

   ```python
   # ëª¨ë¸ í•™ìŠµ
   model.fit(x_data, t_data)
   
   # Weight, bias ì¶œë ¥
   print('W: {}, b: {}'.format(model.coef_, model.intercept_)) # W: [[2.]], b: [1.]
   ```

4. **Prediction**

   ```python
   print(model.predict([[9]])) # [[19.]]
   ```

------

Reference: [ML_0224.ipynb](https://github.com/sammitako/TIL/blob/master/Machine%20Learning/source-code/ML_0224.ipynb)

