# Multinomial Classification

## Classification ì •ì˜

ì´ˆí‰ë©´(hyperplane)ì„ êµ¬í•´ì„œ, ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì–´ëŠ ì˜ì—­ì— ì˜ˆì¸¡í•  ë°ì´í„°ê°€ ë“¤ì–´ê°€ëŠ” ì§€ íŒë‹¨í•œë‹¤.

ì´ˆí‰ë©´ì€ ë…ë¦½ë³€ìˆ˜ì˜ ê°œìˆ˜, ì¦‰ ì°¨ì›ì— ë”°ë¼ ì„ , ë©´, ê³µê°„ì´ ë  ìˆ˜ ìˆë‹¤.

- **Binary Classification**

  : 0(False) ë˜ëŠ” 1(True)ì„ ì˜ˆì¸¡í•˜ëŠ” ê¸°ë²•

  - 
    Logistic Regression

  : ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ í‘œí˜„í•˜ëŠ” ì§ì„ ì„ ì°¾ì€ í›„, ì´ êµ¬ë¶„ì„ ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚´ê°€ ì˜ˆì¸¡í•˜ë ¤ëŠ” ë°ì´í„°ê°€ ì–´ëŠ ìª½ ì˜ì—­ì— ë“¤ì–´ê°€ëŠ” ì§€ë¥¼ íŒë‹¨

  <details>
    <summary>Logistic Regression Graph</summary>
    <img src="md-images/LogitGraph.png">
  </details>

  <br>

- **Multinomial Classification**

  : ì—¬ëŸ¬ ê°œì˜ ë¶„ë¥˜(ë…ë¦½ë³€ìˆ˜) ì¤‘ì— ì–´ë–¤ ë¶„ë¥˜ì— ì†í•˜ëŠ” ì§€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê¸°ë²•

  ì—¬ëŸ¬ ê°œì˜ ë…ë¦½ë³€ìˆ˜ë¥¼ ê·¸ë˜í”„ë¡œ í‘œí˜„í•  ì‹œ 3ì°¨ì› ê³µê°„ì— ë°ì´í„°(ì )ê°€ ì°íˆê²Œ ëœë‹¤.

  ë”°ë¼ì„œ, ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ í‘œí˜„í•˜ëŠ” í‰ë©´ì„ ë¨¼ì € ì°¾ì€ í›„ ê·¸ í‰ë©´ì„ ê¸°ì¤€ìœ¼ë¡œ ì ì´ ì–´ë””ì— ìœ„ì¹˜í•˜ëŠ” ì§€ë¥¼ íŒë‹¨í•œë‹¤.
  
  <br>

**ì§€ê¸ˆê¹Œì§€ ë°°ìš´ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²• ì¤‘ ì–´ëŠ ê²ƒì„ ì‚¬ìš©í•´ì•¼ í• ê¹Œ?**

Labelì˜ í˜•íƒœë¥¼ ë³´ë©´ ì•Œ ìˆ˜ ìˆë‹¤.

- ì—°ì†ì ì¸ ìˆ«ìê°’ - Linear Regression

- ë‘˜ ì¤‘ì˜ í•˜ë‚˜ - Binary Classification

- ì—¬ëŸ¬ ê°œ ì¤‘ì˜ í•˜ë‚˜ - Multinomial Classification 

  <br>

## Multinomial Classification


Binary Classificationì„ ì—¬ëŸ¬ ê°œ ëª¨ì€ í›„ ê·¸ ì¤‘ í™•ë¥ ê°’ì´ ê°€ì¥ ë†’ì€ ì˜ì—­ìœ¼ë¡œ ë¶„ë¥˜í•˜ë©´ ëœë‹¤.

ë‹¤ì‹œ ë§í•´, ì´ì§„ ë¶„ë¥˜ë¥¼ ì—¬ëŸ¬ ë²ˆí•  ê²½ìš° ê°ê°ì˜ í™•ë¥ ë“¤ì´ êµ¬í•´ì§€ëŠ” ë°, ê·¸ ì¤‘ í™•ë¥ ê°’ì´ ê°€ì¥ ë†’ì€ ê²ƒìœ¼ë¡œ ì±„íƒí•˜ë©´ ëœë‹¤. <br>



**ì˜ˆ) ì‹œí—˜ì„±ì ê³¼ ì¶œì„ì ìˆ˜ì— ë”°ë¥¸ ì„±ì ë“±ê¸‰**

ì´ì§„ ë¶„ë¥˜ 3ê°œë¥¼ ëª¨ì•„ì„œ Multinomial Classificationì„ í‘œí˜„

![image-20210305200756565](md-images/multinomial-classification.png)

ì´ ì˜ˆì œì—ì„œëŠ” Labelì˜ ì¢…ë¥˜ê°€ ì´ 3ê°œ ì´ë¯€ë¡œ Logistic Regressionì€ ì´ 3ê°œë¥¼ ë§Œë“¤ì–´ì•¼ í•œë‹¤.

ì´ì œ, ê°ê°ì˜ Logistic Regressionì„ í–‰ë ¬ë¡œ í‘œí˜„í•´ ë³´ì. <br>



í•™ìŠµìš© ë°ì´í„°ì˜ ë…ë¦½ë³€ìˆ˜ê°€ 2ê°œ ì´ë¯€ë¡œ ì„  A, B, C ëª¨ë‘ WëŠ” 2ê°œ, bëŠ” 1ê°œê°€ í•„ìš”í•˜ë‹¤.

ë”°ë¼ì„œ, ê° ì„ ì˜ WëŠ” 2í–‰ 1ì—´ì´ ë˜ì•¼ í•œë‹¤.

![image-20210305200840648](md-images/formula-1.png)

í”„ë¡œê·¸ë¨ì€ ì„  A, B, Cë¥¼ í–‰ë ¬ ì—°ì‚°ì„ í†µí•´ í•˜ë‚˜ì˜ í–‰ë ¬ì‹ìœ¼ë¡œ í‘œí˜„í•  ê²ƒì´ë‹¤. ì—¬ê¸°ê¹Œì§€ê°€ Linear Regressionì— ëŒ€í•œ í–‰ë ¬ì—°ì‚°ì´ë‹¤.

![image-20210305200911711](md-images/formula-2.png)

ì´ë ‡ê²Œ êµ¬í•œ Linear Regression ì§ì„ ì„ Logistic Regressionìœ¼ë¡œ ë³€í™˜ì‹œí‚¤ê¸° ìœ„í•´ ìœ„ì˜ í–‰ë ¬ì‹ì„ Sigmoid í•¨ìˆ˜ì— ë„£ì–´ Cross-Entropy, ì¦‰ ì†ì‹¤í•¨ìˆ˜ë¥¼ ë§Œë“ ë‹¤.

(ì°¸ê³ : Sigmoid í•¨ìˆ˜ëŠ” ê° ë°ì´í„°ì— ëŒ€í•´ 0~1 ì‚¬ì´ì˜ ê°œë³„ì  í™•ë¥ ê°’ìœ¼ë¡œ ë°˜í™˜í•œë‹¤.) <br>



Sigmoid í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ë‚˜ì˜¨ Cross Entropy, ì¦‰ log loss(ì†ì‹¤í•¨ìˆ˜)ì˜ ê°’ì„ ì¤„ì´ë©´ì„œ Gradient Descent Algorithm(ê²½ì‚¬í•˜ê°•ë²•)ì„ í†µí•´ ìµœì ì˜ W, bë¥¼ ì°¾ì•„ ëª¨ë¸ì„ ì™„ì„±ì‹œí‚¨ í›„ ì˜ˆì¸¡ê°’(í™•ë¥ ê°’)ì„ ì°¾ëŠ”ë‹¤.

ìµœì¢…ì ìœ¼ë¡œ ì˜ˆì¸¡ì„ ìœ„í•œ í–‰ë ¬ ì—°ì‚°ì˜ ê²°ê³¼ë¡œ, ê° ì„ ì— ëŒ€í•œ í™•ë¥ ê°’ì´ ë„ì¶œë  ê²ƒì´ë‹¤.

![image-20210305200934681](md-images/answer-1.png)

ì´ ì¤‘, ê°€ì¥ í° ê°’ì˜ í™•ë¥ ì„ ê°€ì§€ëŠ” Bê°€ ì˜ˆì¸¡ ê²°ê³¼ê°’ìœ¼ë¡œ ì±„íƒë  ê²ƒì´ë‹¤.

<br>

### **ì‚¬ì‹¤! Multinomial Classificationì—ì„œëŠ”**

**í™œì„±í™” í•¨ìˆ˜(Activation Function)**ë¡œ Sigmoid í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ , **Softmax í•¨ìˆ˜**ë¥¼ ì‚¬ìš©í•œë‹¤.

(ì°¸ê³ : Binary Classificationì—ì„œëŠ” Logistic Regressionì„ í†µí•´ ì˜ˆì¸¡ ê²°ê³¼ê°’ **1ê°œ**ë¥¼ êµ¬í•  ë•ŒëŠ” í™œì„±í™” í•¨ìˆ˜ë¡œ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í–ˆë‹¤.) <br>



í™œì„±í™” í•¨ìˆ˜ê°€ ë°”ë€Œë¯€ë¡œ Convex í•¨ìˆ˜ë¡œ ë°”ê¿”ì£¼ëŠ” ì†ì‹¤í•¨ìˆ˜ë„ ë˜í•œ ë°”ë€Œê²Œ ë˜ê³  ìš°ë¦¬ê°€ í•™ìŠµì„ í†µí•´ ìµœì¢…ì ìœ¼ë¡œ ë§Œë“¤ì–´ì•¼ í•˜ë„ ëª¨ë¸ ìì²´ë„ ë°”ë€Œê²Œ ëœë‹¤.

ë”°ë¼ì„œ t_tableì— ë“¤ì–´ìˆëŠ” ê° ë¶„ë¥˜ ì¢…ë¥˜ì— ëŒ€í•œ í™•ë¥ ê°’ì´ ì•„ë‹ˆë¼, ì „ì²´ labelì— ëŒ€í•´ í™•ë¥ ê°’ì„ 1ë¡œ ë‘ê³ , ê·¸ ì•ˆì—ì„œì˜ ê° labelì— ëŒ€í•œ í™•ë¥ ê°’ì´ ë„ì¶œëœë‹¤.

ì¦‰, ì „ì²´ í™•ë¥ ê°’ `1` ì•ˆì—ì„œ ê° label ì¢…ë¥˜ì— ëŒ€í•œ í™•ë¥ ê°’ì´ ë‚˜ë‰˜ê²Œ ëœë‹¤.

![image-20210305201135529](md-images/final-answer.png)

<br>

**Softmax function**

![image-20210305201201474](md-images/formula-3.png)

**Cross Entropy function**

![image-20210305201227605](md-images/formula-final.png)



ê²°ê³¼ì ìœ¼ë¡œ í–‰ë ¬ì‹ ë³€ê²½ ë° ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ë¥¼ í†µí•œ ì†ì‹¤í•¨ìˆ˜ë¡œ ëª¨ë¸ì„ ë§Œë“¤ë©´ ê° ë¶„ë¥˜ì— ëŒ€í•œ í™•ë¥ ê°’ì´ ë‚˜ì˜¨ë‹¤.

**âœ‹ğŸ» WAIT,WAIT,**

**ì†ì‹¤í•¨ìˆ˜ê°€ t_labelì„ ê°€ì§€ê³  í–‰ë ¬(y) ì—°ì‚°ì„ í†µí•´ ë°ì´í„°ë¥¼ í•™ìŠµì‹œì¼œì•¼ í•˜ëŠ”ë° ì–´ë–¡í•˜ì§€?**

<br>

## One-Hot Encoding


1ì°¨ì› ë²”ì£¼í˜• ë°ì´í„°ë¥¼ 2ì°¨ì› ìˆ˜ì¹˜ í–‰ë ¬ë¡œ í‘œí˜„í•˜ëŠ” ë°©ë²•.

ì¦‰, ë ˆì´ë¸”(t)ì„ ì»¬ëŸ¼ìœ¼ë¡œ ë†“ê³  í•´ë‹¹ ì»¬ëŸ¼ì— í•´ë‹¹ë˜ëŠ” ì¢…ë¥˜ì¼ ê²½ìš° 1ì„, ì•„ë‹Œ ì¢…ë¥˜ì—ëŠ” 0ìœ¼ë¡œ í‘œí˜„.

ìœ„ì˜ ì˜ˆì— ë”°ë¥´ë©´,

í•™ìŠµìš© ë°ì´í„°(5í–‰ 3ì—´)ì— ëŒ€í•´ One-Hot Encodingì„ í•´ì£¼ë©´ í•™ìŠµìš© ë°ì´í„°ì˜ ë ˆì´ë¸”(t) ì¢…ë¥˜ì˜ ê°œìˆ˜(3ê°œ)ì™€ ë™ì¼í•˜ê²Œ 3ì—´ë¡œ ë§Œë“¤ì–´ì¤€ë‹¤.

![image-20210305201309863](md-images/one-hot.png)

<br>

## Softmax Regression - BMI ì½”ë“œ êµ¬í˜„

ì €ì²´ì¤‘, ì •ìƒ, ê³¼ì²´ì¤‘, ë¹„ë§Œ

![image-20210305201340416](md-images/bmi.png)

## Data Preprocessing


- ê²°ì¸¡ì¹˜, ì´ìƒì¹˜

- One-Hot Encoding

  Tensorflow ê¸°ëŠ¥ì„ ì´ìš©í•´ì„œ One-Hot Encodingì„ ìƒì„±, ì´ ë•Œ Tensorflowì˜ ë…¸ë“œ(`tf.one_hot()`)ë¥¼ ìƒì„±í•´ì„œ ì‹¤í–‰í•˜ëŠ” ë°©ë²•ì´ë¯€ë¡œ ì„¸ì…˜ì´ í•„ìš”í•˜ë‹¤.

  Tensorflow ë…¸ë“œë¥¼ ì‹¤í–‰í•˜ë©´ ndarray í˜•íƒœë¡œ ì¶œë ¥ì´ ë˜ë¯€ë¡œ ì„¸ì…˜ì„ ì´ìš©í•´ì„œ ì‹¤í–‰ í›„ ì›í•«ì¸ì½”ë”© í˜•íƒœì˜ ë°ì´í„°ë¥¼ ë°˜í™˜ ë°›ìœ¼ë©´ ëœë‹¤.

  íŒŒë¼ë¯¸í„° ê°’ìœ¼ë¡œ ë„˜ê²¨ì§€ëŠ” `depth`ëŠ” ë ˆì´ë¸” ì¢…ë¥˜ì˜ ê°¯ìˆ˜ì´ë‹¤.

- ì •ê·œí™”

```python
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt

from scipy import stats
from sklearn.preprocessing import MinMaxScaler

df = pd.read_csv('data/bmi.csv', skiprows=3)
display(df.head(), df.shape)

# ê²°ì¸¡ì¹˜ í™•ì¸
df.isnull().sum()

# ì´ìƒì¹˜ í™•ì¸
fig = plt.figure()
fig_1 = fig.add_subplot(1,3,1)
fig_2 = fig.add_subplot(1,3,2)
fig_3 = fig.add_subplot(1,3,3)

fig_1.boxplot(df['label'])
fig_2.boxplot(df['height'])
fig_3.boxplot(df['weight'])

fig.tight_layout()
plt.show()

# Training Data Set
x_data = df[['height', 'weight']].values 
t_data = df['label'].values

# ì •ê·œí™”
scaler_x = MinMaxScaler()
scaler_x.fit(x_data)
norm_x_data = scaler_x.transform(x_data)
print(norm_x_data)

# One-Hot Encoding
sess = tf.Session()
norm_t_data = sess.run(tf.one_hot(t_data, depth=3))
print(norm_t_data)
```
<br>

## Tensorflow


- Data Split for Learning and Validation

```python
print(norm_x_data.shape)
train_x_data = norm_x_data[:int(norm_x_data.shape[0] * 0.7)]
val_x_data = norm_x_data[int(norm_x_data.shape[0] * 0.7):]
print(train_x_data.shape)
print(val_x_data.shape)

train_t_data = norm_t_data[:int(norm_t_data.shape[0] * 0.7)]
val_t_data = norm_t_data[int(norm_t_data.shape[0] * 0.7):]
print(train_t_data.shape)
print(val_t_data.shape)
```

- Softmax Function
- Cross Entropy Function

```python
# X, T
X = tf.placeholder(shape=[None, 2], dtype=tf.float32) # 2: height, weight
T = tf.placeholder(shape=[None, 3], dtype=tf.float32) # one hot encoding í˜•íƒœ

# W, b
W = tf.Variable(tf.random.normal([2,3]), name='weight')
b = tf.Variable(tf.random.normal([3]), name='bias')

# Hypothesis
logit = tf.matmul(X, W) + b
H = tf.nn.softmax(logit) # Softmax Activation Function ì´ìš©

# loss
loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=T))

# gradient descent algorithm
train = tf.train.GradientDescentOptimizer(learning_rate=1e-4).minimize(loss)

# learning
sess.run(tf.global_variables_initializer())

for step in range(300000):
    _, W_val, b_val, loss_val = sess.run([train, W, b, loss], feed_dict={X: train_x_data, T: train_t_data})
    
    if step % 30000 == 0:
        print('W: {}, b: {}, loss: {}'.format(W_val, b_val, loss_val))
```

<br>
## Model Accuracy


ì˜ˆì¸¡í•œ í™•ë¥ ì—ì„œ ê°€ì¥ ë†’ì€ í™•ë¥ ê°’

```python
predict = tf.argmax(H, axis=1)
correct = tf.equal(predict, tf.argmax(T, axis=1)) 
for_acc = tf.cast(correct, dtype=tf.float32) 

# Accuracy
accuracy = tf.reduce_mean(for_acc)
accuracy_val = sess.run(accuracy, feed_dict={X: val_x_data, T: val_t_data})

print('ëª¨ë¸ì˜ ì •í™•ë„: {}'.format(accuracy_val)) # 80%
```
<br>
## Prediction


ê° Label ì¢…ë¥˜ì— ëŒ€í•œ ì˜ˆì¸¡ í™•ë¥ ê°’ ì¤‘ ê°€ì¥ í° ê°’ìœ¼ë¡œ ë¶„ë¥˜

```python
height = 187
weight = 78
my_state = [[height, weight]]

result = sess.run(H, feed_dict={X: scaler_x.transform(my_state)})
print(result) # ì •ìƒ, [0: ì €ì²´ì¤‘, 1: ì •ìƒ, 2: ê³¼ì²´ì¤‘]
```


<br>
-----


Reference: [ML_0305](https://github.com/sammitako/TIL/blob/master/Machine%20Learning/source-code/ML_0305.ipynb)

